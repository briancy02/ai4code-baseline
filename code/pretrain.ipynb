{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e720f9bd-66dc-41eb-9335-ef84f70cc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "#from easynmt import EasyNMT\n",
    "from tqdm import tqdm\n",
    "#model = EasyNMT('opus-mt')\n",
    "class PretrainDataset(Dataset):\n",
    "    # train mark is taken as input - train mark contains markdown cells\n",
    "    def __init__(self, df, training_corpus, model_name_or_path, total_max_len, md_max_len, fts):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.md_max_len = md_max_len\n",
    "        self.total_max_len = total_max_len  # maxlen allowed by model config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.fts = fts\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        #print(\"row source\", row.source)\n",
    "        #print(\"code\", [str(x) for x in self.fts[row.id][\"codes\"]])\n",
    "        #text = model.translate(row.source, target_lang='en')\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            row.source,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            # 64\n",
    "            max_length=self.md_max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        n_md = self.fts[row.id][\"total_md\"]\n",
    "        n_code = self.fts[row.id][\"total_code\"]\n",
    "        #print(n_md, n_code)\n",
    "        #print(\"one code cell\", len(self.fts[row.id][\"codes\"][0]))\n",
    "        code_inputs = self.tokenizer.batch_encode_plus(\n",
    "            [str(x) for x in self.fts[row.id][\"codes\"]],\n",
    "            # Whether or not to encode the sequences with the special tokens relative to their model.\n",
    "            add_special_tokens=True,\n",
    "            # Truncate to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided. This will truncate token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "            max_length=19,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        #print(len(code_inputs))\n",
    "#         features[idx][\"total_code\"] = total_code\n",
    "#         features[idx][\"total_md\"] = total_md\n",
    "#         features[idx][\"codes\"] = codes\n",
    "        \n",
    "        if n_md + n_code == 0:\n",
    "            fts = torch.FloatTensor([0])\n",
    "        else:\n",
    "            fts = torch.FloatTensor([n_md / (n_md + n_code)])\n",
    "        # fts is percentage of md out of all tokens?    \n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        # numerical representations of tokens building the sequences that will be used as input by the model\n",
    "        #print(\"fts\", fts)\n",
    "        #print(\"ids\", ids)\n",
    "        for x in code_inputs['input_ids']:\n",
    "            #print(x)\n",
    "            ids.extend(x[:-1])\n",
    "        \n",
    "        #print(\"ids\", ids)\n",
    "        ids = ids[:self.total_max_len]\n",
    "        if len(ids) != self.total_max_len:\n",
    "            ids = ids + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(ids))\n",
    "        ids = torch.LongTensor(ids)\n",
    "        \n",
    "        # https://huggingface.co/docs/transformers/glossary#attention-mask\n",
    "        mask = inputs['attention_mask']\n",
    "        for x in code_inputs['attention_mask']:\n",
    "            mask.extend(x[:-1])\n",
    "        mask = mask[:self.total_max_len]\n",
    "        if len(mask) != self.total_max_len:\n",
    "            mask = mask + [self.tokenizer.pad_token_id, ] * (self.total_max_len - len(mask))\n",
    "        mask = torch.LongTensor(mask)\n",
    "\n",
    "        assert len(ids) == self.total_max_len\n",
    "\n",
    "        ##### MASKING FOR PRETRAINING\n",
    "        #ids, mask, fts, torch.FloatTensor([row.pct_rank])\n",
    "        labels = ids.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability` 0.15)\n",
    "        probability_matrix = torch.full(labels.shape, 0.15)\n",
    "#         special_tokens_mask = [\n",
    "#                 self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=False) for val in labels.tolist()\n",
    "#         ]\n",
    "#         special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "\n",
    "        #probability_matrix.masked_fill_(mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        ids[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
    "        ids[indices_random] = random_words[indices_random]\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return ids, mask, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09204317-cd31-47a8-b558-78054b834c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CONFIGS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3317222/191652273.py:45: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df_orders = pd.read_csv(\n",
      "Reusing dataset code_search_net (/home/briancy2/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bdadf268414a53b67a8522fb14f96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "from model import *\n",
    "from tqdm import tqdm\n",
    "import sys, os\n",
    "from metrics import *\n",
    "import torch\n",
    "import argparse\n",
    "from stlr import SlantedTriangular\n",
    "data_dir = str(Path.cwd()) + '/data/'\n",
    "\n",
    "model_name_or_path='microsoft/codebert-base'\n",
    "train_mark_path=data_dir+ 'train_mark.csv'\n",
    "train_features_path=data_dir+ 'train_fts.json'\n",
    "val_mark_path=data_dir+ 'val_mark.csv'\n",
    "val_features_path=data_dir+ 'val_fts.json'\n",
    "val_path=data_dir+ 'val.csv'\n",
    "checkpoint_format=\"./outputs/model-{e}.bin\"\n",
    "\n",
    "num_gpus=1\n",
    "md_max_len=64\n",
    "total_max_len=512\n",
    "batch_size=4\n",
    "accumulation_steps=4\n",
    "epochs=5\n",
    "n_workers=4\n",
    "\n",
    "#os.mkdir(\"./outputs\")\n",
    "\n",
    "print(\"MODEL CONFIGS\")\n",
    "\n",
    "train_df_mark = pd.read_csv(train_mark_path).drop(\"parent_id\", axis=1).dropna().reset_index(drop=True)\n",
    "train_fts = json.load(open(train_features_path))\n",
    "val_df_mark = pd.read_csv(val_mark_path).drop(\"parent_id\", axis=1).dropna().reset_index(drop=True)\n",
    "val_fts = json.load(open(val_features_path))\n",
    "val_df = pd.read_csv(val_path)\n",
    "\n",
    "order_df = pd.read_csv(data_dir+\"train_orders.csv\").set_index(\"id\")\n",
    "df_orders = pd.read_csv(\n",
    "    data_dir + 'train_orders.csv',\n",
    "    index_col='id',\n",
    "    squeeze=True,\n",
    ").str.split()\n",
    "\n",
    "raw_datasets = load_dataset(\"code_search_net\", \"python\")\n",
    "def get_training_corpus():\n",
    "    return (\n",
    "        raw_datasets[\"train\"][i : i + 1000][\"whole_func_string\"]\n",
    "        for i in range(0, len(raw_datasets[\"train\"]), 1000)\n",
    "    )\n",
    "\n",
    "# takes in df \n",
    "training_corpus = get_training_corpus()\n",
    "train_ds = PretrainDataset(train_df_mark, training_corpus, model_name_or_path=model_name_or_path, md_max_len=md_max_len,\n",
    "                           total_max_len=total_max_len, fts=train_fts)\n",
    "def collate_fn_padd(batch):\n",
    "    '''\n",
    "    Padds batch of variable length\n",
    "\n",
    "    note: it converts things ToTensor manually here since the ToTensor transform\n",
    "    assume it takes in images rather than arbitrary tensors.\n",
    "    '''\n",
    "    inputs = torch.nn.utils.rnn.pad_sequence([ t[0] for t in batch], batch_first=True)\n",
    "    mask = torch.nn.utils.rnn.pad_sequence([ t[1] for t in batch], batch_first=True)\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([ t[2] for t in batch], batch_first=True)\n",
    "    return inputs, mask, labels\n",
    "\n",
    "val_ds = PretrainDataset(val_df_mark, training_corpus, model_name_or_path=model_name_or_path, md_max_len=md_max_len,\n",
    "                         total_max_len=total_max_len, fts=val_fts)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=n_workers,\n",
    "                          pin_memory=False, drop_last=True, collate_fn = collate_fn_padd)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=n_workers,\n",
    "                        pin_memory=False, drop_last=False,collate_fn = collate_fn_padd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e115a55-c33d-45cb-a97b-21ef9b55d09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 10431,  2907,  ...,     1,     1,     1],\n",
      "        [    0, 48134,  4317,  ...,     1,     1,     1],\n",
      "        [    0, 48342,  7192,  ...,     1,     1, 50264],\n",
      "        [    0, 10431,  7435,  ..., 50264,     1,     1]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([0.1484, 0.1562, 0.1328, 0.1465])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_loader))\n",
    "print(batch[0])\n",
    "print(batch[1])\n",
    "print((batch[1] == batch[0]).sum(axis=1)/512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e76ac1-4fde-4121-a5ef-583405023348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LongformerModel, LongformerTokenizer, LongformerForMaskedLM\n",
    "class PretrainingModel(nn.Module):\n",
    "    def __init__(self, model_path, md_max_len):\n",
    "        super(PretrainingModel, self).__init__()\n",
    "        self.attention_window = 512\n",
    "        self.md_max_len = md_max_len\n",
    "        self.max_input_len = 512\n",
    "        self.max_input_len += 2\n",
    "        # lengthen model\n",
    "        self.model = AutoModel.from_pretrained(model_path)\n",
    "        config = LongformerConfig(vocab_size = self.model.config.vocab_size, max_position_embeddings = self.model.config.max_position_embeddings)\n",
    "        #config.attention_mode = 'sliding_chunks'\n",
    "        longformer_model_MLM = LongformerForMaskedLM(config=config)\n",
    "        longformer_model = longformer_model_MLM.longformer\n",
    "        print(config)\n",
    "        current_max_input_len, embed_size = self.model.embeddings.position_embeddings.weight.shape\n",
    "#         print(current_max_input_len, embed_size)\n",
    "        new_encoder_pos_embed = self.model.embeddings.position_embeddings.weight.new_empty(self.max_input_len, embed_size)\n",
    "        print(\"new embed size\", new_encoder_pos_embed.size())\n",
    "        k = 2\n",
    "        step = current_max_input_len - 2\n",
    "        while k < self.max_input_len - 1:\n",
    "            new_encoder_pos_embed[k:(k+step)] = self.model.embeddings.position_embeddings.weight[2:]\n",
    "            k += step\n",
    "        longformer_model.embeddings.position_embeddings.weight.data = new_encoder_pos_embed\n",
    "        \n",
    "        #Attention set up\n",
    "        longformer_model.config.vocab_size = self.model.config.vocab_size\n",
    "        longformer_model.config.layer_norm_eps = self.model.config.layer_norm_eps\n",
    "        longformer_model.config.attention_window = [self.attention_window] * self.model.config.num_hidden_layers\n",
    "#         longformer_model.config.attention_window[:4] = [32,32,64,64]\n",
    "#         longformer_model.config.attention_window[4:6] = [128, 128]\n",
    "#         longformer_model.config.attention_window[6:8] = [256,256]\n",
    "#         longformer_model.config.attention_window[8:10] = [512, 512]\n",
    "        #print(self.model.config.num_hidden_layers)\n",
    "        #print(self.model.config.attention_window)\n",
    "        \n",
    "        for i, layer in enumerate(self.model.encoder.layer):\n",
    "            longformer_self_attn_for_codebert = LongformerSelfAttention(longformer_model.config, layer_id=i)\n",
    "            longformer_self_attn_for_codebert.query = layer.attention.self.query\n",
    "            longformer_self_attn_for_codebert.key = layer.attention.self.key\n",
    "            longformer_self_attn_for_codebert.value = layer.attention.self.value\n",
    "            \n",
    "            longformer_self_attn_for_codebert.query_global = copy.deepcopy(layer.attention.self.query)\n",
    "            longformer_self_attn_for_codebert.key_global = copy.deepcopy(layer.attention.self.key)\n",
    "            longformer_self_attn_for_codebert.value_global = copy.deepcopy(layer.attention.self.value)\n",
    "            \n",
    "            longformer_model.encoder.layer[i].attention.self = longformer_self_attn_for_codebert\n",
    "            longformer_model.encoder.layer[i].attention.output.dense = layer.attention.output.dense\n",
    "            \n",
    "            longformer_model.encoder.layer[i].intermediate.dense = layer.intermediate.dense\n",
    "            \n",
    "            longformer_model.encoder.layer[i].output.dense = layer.output.dense\n",
    "\n",
    "        self.model =  longformer_model_MLM\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, masked_mlm_labels):\n",
    "        x = self.model(input_ids= input_ids, attention_mask = attention_mask, labels = masked_mlm_labels)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e42f50-6d2b-40e4-a26d-e308508eccc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate(model, val_loader):\n",
    "    model.eval()\n",
    "\n",
    "    tbar = tqdm(val_loader, file=sys.stdout)\n",
    "\n",
    "    preds = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(tbar):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                # inputs dim is 4\n",
    "                loss, pred, _, _ = model(input_ids = data[0], attention_mask = data[1], masked_mlm_labels = data[2])\n",
    "\n",
    "            labels.append(data[2].detach().cpu().numpy().ravel())\n",
    "            preds.append(pred.detach().cpu().numpy().ravel())\n",
    "\n",
    "    return np.concatenate(labels), np.concatenate(data[2])\n",
    "\n",
    "def train(model, train_loader, val_loader, epochs):\n",
    "    np.random.seed(0)\n",
    "    # Creating optimizer and lr schedulers\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    num_train_optimization_steps = int(epochs * len(train_loader) / accumulation_steps)\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5,\n",
    "                      correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
    "    #scheduler = SlantedTriangular(optimizer,epochs,num_steps_per_epoch=num_train_optimization_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.05 * num_train_optimization_steps,\n",
    "                                                num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n",
    "\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    resume_from_epoch = 0\n",
    "    for try_epoch in range(epochs, 0, -1):\n",
    "        if os.path.exists('./outputs/model-{epoch}.bin'.format(epoch=try_epoch)):\n",
    "            resume_from_epoch = try_epoch+1\n",
    "            break\n",
    "    if resume_from_epoch:\n",
    "        filepath = checkpoint_format.format(e=resume_from_epoch)\n",
    "        checkpoint = torch.load(checkpoint_format.format(e=try_epoch))\n",
    "        model.load_state_dict(checkpoint)\n",
    "        \n",
    "    for e in range(resume_from_epoch, epochs):\n",
    "        model.train()\n",
    "        tbar = tqdm(train_loader, file=sys.stdout)\n",
    "        loss_list = []\n",
    "        preds = []\n",
    "\n",
    "        for idx, data in enumerate(tbar):\n",
    "            with torch.cuda.amp.autocast():\n",
    "                loss = model(input_ids = data[0].cuda(), attention_mask = data[1].cuda(), masked_mlm_labels = data[2].cuda()).to_tuple()[0]\n",
    "                scaler.scale(loss).backward()\n",
    "            if idx % accumulation_steps == 0 or idx == len(tbar) - 1:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "            loss_list.append(loss.detach().cpu().item())\n",
    "            #preds.append(pred.detach().cpu().numpy().ravel())\n",
    "\n",
    "            avg_loss = np.round(np.mean(loss_list), 4)\n",
    "\n",
    "            tbar.set_description(f\"Epoch {e + 1} Loss: {avg_loss} lr: {optimizer.param_groups[0]['lr']}\")\n",
    "        \n",
    "        # objective is to learn the percentage ranking\n",
    "        #y_val, y_pred = validate(model, val_loader)\n",
    "        #print(\"Preds score\", y_val - y_pred)\n",
    "        torch.save(model.state_dict(), checkpoint_format.format(e=e))\n",
    "\n",
    "    return model, y_pred\n",
    "\n",
    "\n",
    "model = PretrainingModel(model_name_or_path, md_max_len)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model, device_ids=[i for i in range(num_gpus)])\n",
    "model = model.to(torch.device(\"cuda\"))\n",
    "model, y_pred = train(model, train_loader, val_loader, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f15ae9-0151-45da-a0b1-41eb0c6ca09e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/briancy2/ai4code-baseline\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb82a9d1-9bd9-43bd-96b0-7c0a7dc8ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CONFIGS\n",
      "/home/briancy2/ai4code-baseline/code/pretrain.py:156: FutureWarning: The squeeze argument has been deprecated and will be removed in a future version. Append .squeeze(\"columns\") to the call to squeeze.\n",
      "\n",
      "\n",
      "  df_orders = pd.read_csv(\n",
      "Reusing dataset code_search_net (/home/briancy2/.cache/huggingface/datasets/code_search_net/python/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:00<00:00, 31.85it/s]\n",
      "LongformerConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"longformer\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"sep_token_id\": 2,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "new embed size torch.Size([1026, 768])\n",
      "/home/briancy2/.conda/envs/ai4code/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|                                                | 0/487528 [00:00<?, ?it/s]/home/briancy2/.conda/envs/ai4code/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "Epoch 1 Loss: 10.957 lr: 2.067573554749676e-08:   0%| | 83/487528 [00:21<33:05:0^C\n",
      "Epoch 1 Loss: 10.957 lr: 2.067573554749676e-08:   0%| | 83/487528 [00:21<35:41:1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/briancy2/ai4code-baseline/code/pretrain.py\", line 340, in <module>\n",
      "    model, y_pred = train(model, train_loader, val_loader, epochs=epochs)    \n",
      "  File \"/home/briancy2/ai4code-baseline/code/pretrain.py\", line 312, in train\n",
      "    scaler.scale(loss.mean()).backward()\n",
      "  File \"/home/briancy2/.conda/envs/ai4code/lib/python3.9/site-packages/torch/_tensor.py\", line 307, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)\n",
      "  File \"/home/briancy2/.conda/envs/ai4code/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 154, in backward\n",
      "    Variable._execution_engine.run_backward(\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 code/pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5111f97-d235-4abc-b65c-64e281c64a87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-ai4code]",
   "language": "python",
   "name": "conda-env-.conda-ai4code-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
